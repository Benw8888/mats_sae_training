{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import plotly.express as px\n",
    "from transformer_lens import utils\n",
    "from datasets import load_dataset\n",
    "from typing import  Dict\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from collections.abc import Generator\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from typing import Any, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import ortho_group\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from fastcluster import linkage\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import wandb\n",
    "from sae_training.activations_store import ActivationsStore\n",
    "from sae_training.evals import run_evals\n",
    "from sae_training.optim import get_scheduler\n",
    "from sae_training.sparse_autoencoder import SparseAutoencoder\n",
    "from sae_training.sae_group import SAEGroup\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "from sae_analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\" \n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by downloading them from huggingface\n",
    "from huggingface_hub import hf_hub_download\n",
    "REPO_ID = \"Benw8888/lp_saes\"\n",
    "layer = 6 # any layer from 0 - 11 works here\n",
    "uuid_str = \"c0es5ci0\"\n",
    "local_folder = f\"/root/mats_sae_training/checkpoints/{uuid_str}\"\n",
    "# file_strings = [\n",
    "#     \"final_sae_group_EleutherAI_pythia-14m_blocks.3.hook_resid_pre_8192_log_feature_sparsity.pt\",\n",
    "#     \"final_sae_group_EleutherAI_pythia-14m_blocks.3.hook_resid_pre_8192.pt\",\n",
    "# ]\n",
    "# file_strings = [\n",
    "#     \"final_sae_group_gpt2-small_blocks.6.hook_resid_pre_49152_log_feature_sparsity.pt\",\n",
    "#     \"final_sae_group_gpt2-small_blocks.6.hook_resid_pre_49152.pt\",\n",
    "# ]\n",
    "file_strings = [\n",
    "    \"final_sae_group_gpt2-small_blocks.6.hook_resid_pre_12288_log_feature_sparsity.pt\",\n",
    "    \"final_sae_group_gpt2-small_blocks.6.hook_resid_pre_12288.pt\",\n",
    "]\n",
    "fs = file_strings[-1]\n",
    "\n",
    "# FILENAME = f\"{uuid_str}/final_sae_group_gpt2-small_blocks.{layer}.hook_resid_pre_49152.pt\"\n",
    "\n",
    "# path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "path = f\"/root/mats_sae_training/checkpoints/{uuid_str}/{fs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then load the SAE, dataset and model using the session loader\n",
    "model, sae_group, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n",
    "    path = path\n",
    ")\n",
    "log_frequencies = torch.load(f\"/root/mats_sae_training/checkpoints/{uuid_str}/{file_strings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sae in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(f\"{i}: Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(\n",
    "    vecs1: Union[torch.Tensor, torch.nn.parameter.Parameter, npt.NDArray],\n",
    "    vecs2: Union[torch.Tensor, torch.nn.parameter.Parameter, npt.NDArray],\n",
    ") -> np.ndarray:\n",
    "    vecs = [vecs1, vecs2]\n",
    "    for i in range(len(vecs)):\n",
    "        if not isinstance(vecs[i], np.ndarray):\n",
    "            vecs[i] = vecs[i].detach().cpu().numpy()  # type: ignore\n",
    "    vecs1, vecs2 = vecs\n",
    "    normalize = lambda v: (v.T / np.linalg.norm(v, axis=1)).T\n",
    "    vecs1_norm = normalize(vecs1)\n",
    "    vecs2_norm = normalize(vecs2)\n",
    "\n",
    "    return vecs1_norm @ vecs2_norm.T\n",
    "\n",
    "class BatchCorrelationCalculator:\n",
    "    def __init__(self, feature_dim_x, feature_dim_y, device=\"cpu\"):\n",
    "        # Initialize sums needed for correlation calculation for each feature dimension\n",
    "        self.sum_x = torch.zeros(feature_dim_x, device=device)\n",
    "        self.sum_y = torch.zeros(feature_dim_y, device=device)\n",
    "        self.sum_x2 = torch.zeros(feature_dim_x, device=device)\n",
    "        self.sum_y2 = torch.zeros(feature_dim_y, device=device)\n",
    "        self.sum_xy = torch.zeros(\n",
    "            (feature_dim_x, feature_dim_y), device=device\n",
    "        )  # This now becomes a matrix\n",
    "        self.n = 0\n",
    "        self.device = device\n",
    "\n",
    "    def update(self, x_batch, y_batch):\n",
    "        # Ensure input batches are torch tensors\n",
    "        x_batch = x_batch.to(self.device)\n",
    "        y_batch = y_batch.to(self.device)\n",
    "\n",
    "        # Update running sums with a new batch\n",
    "        self.sum_x += torch.sum(x_batch, dim=0)\n",
    "        self.sum_y += torch.sum(y_batch, dim=0)\n",
    "        self.sum_x2 += torch.sum(x_batch**2, dim=0)\n",
    "        self.sum_y2 += torch.sum(y_batch**2, dim=0)\n",
    "        for i in range(x_batch.shape[1]):  # Iterate over features in x\n",
    "            for j in range(y_batch.shape[1]):  # Iterate over features in y\n",
    "                self.sum_xy[i, j] += torch.sum(x_batch[:, i] * y_batch[:, j])\n",
    "        self.n += x_batch.shape[0]\n",
    "\n",
    "    def compute_correlation(self):\n",
    "        # Compute Pearson correlation coefficient matrix between features of the two vectors\n",
    "        numerator = self.n * self.sum_xy - torch.ger(self.sum_x, self.sum_y)\n",
    "        denominator = torch.sqrt(\n",
    "            torch.ger(self.n * self.sum_x2 - self.sum_x**2, self.n * self.sum_y2 - self.sum_y**2)\n",
    "        )\n",
    "\n",
    "        # Handle division by zero for cases with no variance\n",
    "        valid = denominator != 0\n",
    "        correlation_matrix = torch.zeros_like(denominator)\n",
    "        correlation_matrix[valid] = numerator[valid] / denominator[valid]\n",
    "\n",
    "        # Set correlations to 0 where denominator is 0 (indicating no variance)\n",
    "        correlation_matrix[~valid] = 0\n",
    "\n",
    "        return correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Metric Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_log_suffix(cfg, hyperparams):\n",
    "    # Create a mapping from cfg list keys to their corresponding hyperparams attributes\n",
    "    key_mapping = {\n",
    "        \"hook_point_layer\": \"layer\",\n",
    "        \"l1_coefficient\": \"coeff\",\n",
    "        \"lp_norm\": \"l\",\n",
    "        \"lr\": \"lr\"\n",
    "    }\n",
    "\n",
    "    # Generate the suffix by iterating over the keys that have list values in cfg\n",
    "    suffix = \"\".join(f\"_{key_mapping.get(key, key)}{getattr(hyperparams, key, '')}\" \n",
    "                    for key, value in vars(cfg).items() if isinstance(value, list))\n",
    "    return suffix\n",
    "\n",
    "batch_size=sae_group.cfg.train_batch_size\n",
    "feature_sampling_window=sae_group.cfg.feature_sampling_window\n",
    "dead_feature_threshold=sae_group.cfg.dead_feature_threshold\n",
    "use_wandb=False  # sae_group.cfg.log_to_wandb\n",
    "wandb_log_frequency=1  # sae_group.cfg.wandb_log_frequency # should we log faster when evaluating?\n",
    "\n",
    "# Only run evals for less time than training:\n",
    "total_training_tokens = 1_000_000\n",
    "total_training_steps = total_training_tokens // batch_size\n",
    "n_training_steps = 0\n",
    "n_training_tokens = 0\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(project=sae_group.cfg.wandb_project, config=sae_group.cfg, name=f\"eval_{sae_group.cfg.run_name}\")\n",
    "\n",
    "\n",
    "# things to store for each sae:\n",
    "# act_freq_scores, n_forward_passes_since_fired, n_frac_active_tokens, optimizer, scheduler, \n",
    "num_saes = len(sae_group)\n",
    "# track active features\n",
    "\n",
    "act_freq_scores = [\n",
    "    torch.zeros(\n",
    "        sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device\n",
    "    ) for sparse_autoencoder in sae_group\n",
    "]\n",
    "n_forward_passes_since_fired = [\n",
    "    torch.zeros(\n",
    "        sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device\n",
    "    ) for sparse_autoencoder in sae_group\n",
    "]\n",
    "n_frac_active_tokens = [0  for _ in range(num_saes)]\n",
    "# corr_calculator = [\n",
    "#     BatchCorrelationCalculator(sparse_autoencoder.cfg.d_sae, sparse_autoencoder.cfg.d_sae)\n",
    "#     for sparse_autoencoder in sae_group\n",
    "# ]\n",
    "co_occurrences = [\n",
    "    torch.zeros((sparse_autoencoder.cfg.d_sae, sparse_autoencoder.cfg.d_sae), \n",
    "                device=sparse_autoencoder.cfg.device)\n",
    "    for sparse_autoencoder in sae_group\n",
    "]\n",
    "occurrences = [\n",
    "    torch.zeros((sparse_autoencoder.cfg.d_sae), \n",
    "                device=sparse_autoencoder.cfg.device)\n",
    "    for sparse_autoencoder in sae_group\n",
    "]\n",
    "\n",
    "\n",
    "all_layers = sae_group.cfg.hook_point_layer\n",
    "if not isinstance(all_layers, list):\n",
    "    all_layers = [all_layers]\n",
    "    \n",
    "# compute the geometric median of the activations of each layer\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(total=total_training_tokens, desc=\"Evaluating SAE\")\n",
    "    while n_training_tokens < total_training_tokens:\n",
    "        # Do a training step.\n",
    "        layer_acts = activation_store.next_batch()\n",
    "        n_training_tokens += batch_size\n",
    "        \n",
    "        for i, (sparse_autoencoder), in enumerate(sae_group):\n",
    "            hyperparams = sparse_autoencoder.cfg\n",
    "            layer_id = all_layers.index(hyperparams.hook_point_layer)\n",
    "            sae_in = layer_acts[:,layer_id,:]\n",
    "            \n",
    "            sparse_autoencoder.eval()\n",
    "            # Make sure the W_dec is still zero-norm\n",
    "            sparse_autoencoder.set_decoder_norm_to_unit_norm()\n",
    "\n",
    "            # log and then reset the feature sparsity every feature_sampling_window steps\n",
    "            if (n_training_steps + 1) % feature_sampling_window == 0:\n",
    "                feature_sparsity = act_freq_scores[i] / n_frac_active_tokens[i]\n",
    "                log_feature_sparsity = torch.log10(feature_sparsity + 1e-10).detach().cpu()\n",
    "\n",
    "                if use_wandb:\n",
    "                    suffix = wandb_log_suffix(sae_group.cfg, hyperparams)\n",
    "                    wandb_histogram = wandb.Histogram(log_feature_sparsity.numpy())\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            f\"metrics/mean_log10_feature_sparsity{suffix}\": log_feature_sparsity.mean().item(),\n",
    "                            f\"plots/feature_density_line_chart{suffix}\": wandb_histogram,\n",
    "                            f\"sparsity/below_1e-5{suffix}\": (feature_sparsity < 1e-5).sum().item(),\n",
    "                            f\"sparsity/below_1e-6{suffix}\": (feature_sparsity < 1e-6).sum().item(),\n",
    "                        },\n",
    "                        step=n_training_steps,\n",
    "                    )\n",
    "\n",
    "                act_freq_scores[i] = torch.zeros(\n",
    "                    sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device\n",
    "                )\n",
    "                n_frac_active_tokens[i] = 0\n",
    "\n",
    "            ghost_grad_neuron_mask = (\n",
    "                n_forward_passes_since_fired[i] > sparse_autoencoder.cfg.dead_feature_window\n",
    "            ).bool()\n",
    "            \n",
    "\n",
    "            # Forward and Backward Passes\n",
    "            (\n",
    "                sae_out,\n",
    "                feature_acts,\n",
    "                loss,\n",
    "                mse_loss,\n",
    "                l1_loss,\n",
    "                ghost_grad_loss,\n",
    "            ) = sparse_autoencoder(\n",
    "                sae_in,\n",
    "                ghost_grad_neuron_mask,\n",
    "            )\n",
    "            did_fire = (feature_acts > 0).float().sum(-2) > 0\n",
    "            n_forward_passes_since_fired[i] += 1\n",
    "            n_forward_passes_since_fired[i][did_fire] = 0\n",
    "            \n",
    "            # Update auto-correlation tracker\n",
    "            # corr_calculator[i].update(feature_acts, feature_acts)\n",
    "            \n",
    "            # update co-occur\n",
    "            binary_acts = (feature_acts > 0) + 0.0\n",
    "            co_occurrences[i] += ((binary_acts.mT) @ (binary_acts)).sum(dim=0)\n",
    "            occurrences[i] += binary_acts.sum(dim=0)\n",
    "            \n",
    "            # Calculate the sparsities, and add it to a list, calculate sparsity metrics\n",
    "            act_freq_scores[i] += (feature_acts.abs() > 0).float().sum(0)\n",
    "            n_frac_active_tokens[i] += batch_size\n",
    "            feature_sparsity = act_freq_scores[i] / n_frac_active_tokens[i]\n",
    "\n",
    "            if use_wandb and ((n_training_steps + 1) % wandb_log_frequency == 0):\n",
    "                # metrics for currents acts\n",
    "                l0 = (feature_acts > 0).float().sum(-1).mean()\n",
    "\n",
    "                per_token_l2_loss = (sae_out - sae_in).pow(2).sum(dim=-1).squeeze()\n",
    "                total_variance = (sae_in - sae_in.mean(0)).pow(2).sum(-1)\n",
    "                explained_variance = 1 - per_token_l2_loss / total_variance\n",
    "\n",
    "                suffix = wandb_log_suffix(sae_group.cfg, hyperparams)\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        # losses\n",
    "                        f\"losses/mse_loss{suffix}\": mse_loss.item(),\n",
    "                        f\"losses/l1_loss{suffix}\": l1_loss.item()\n",
    "                        / sparse_autoencoder.l1_coefficient,  # normalize by l1 coefficient\n",
    "                        f\"losses/ghost_grad_loss{suffix}\": ghost_grad_loss.item(),\n",
    "                        f\"losses/overall_loss{suffix}\": loss.item(),\n",
    "                        # variance explained\n",
    "                        f\"metrics/explained_variance{suffix}\": explained_variance.mean().item(),\n",
    "                        f\"metrics/explained_variance_std{suffix}\": explained_variance.std().item(),\n",
    "                        f\"metrics/l0{suffix}\": l0.item(),\n",
    "                        # sparsity\n",
    "                        f\"sparsity/mean_passes_since_fired{suffix}\": n_forward_passes_since_fired[i].mean().item(),\n",
    "                        f\"sparsity/dead_features{suffix}\": ghost_grad_neuron_mask.sum().item(),\n",
    "                        f\"details/n_training_tokens{suffix}\": n_training_tokens,\n",
    "                    },\n",
    "                    step=n_training_steps,\n",
    "                )\n",
    "\n",
    "            # record evals more frequently than while training\n",
    "            if use_wandb and ((n_training_steps) % (wandb_log_frequency) == 0):\n",
    "                sparse_autoencoder.eval()\n",
    "                suffix = wandb_log_suffix(sae_group.cfg, hyperparams)\n",
    "                run_evals(sparse_autoencoder, activation_store, model, n_training_steps, suffix=suffix)\n",
    "                sparse_autoencoder.train()\n",
    "                \n",
    "        n_training_steps += 1\n",
    "        # pbar.set_description(\n",
    "        #     f\"{n_training_steps}| MSE Loss {mse_loss.item():.3f} | L1 {l1_loss.item():.3f}\"\n",
    "        # )\n",
    "        pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate jaccard coefficient (co_occur / (either one firing))\n",
    "\n",
    "jaccard = []\n",
    "\n",
    "for i, (sparse_autoencoder), in enumerate(sae_group):\n",
    "    co_occ = co_occurrences[i]\n",
    "    occ = occurrences[i]\n",
    "    jac = co_occ / (occ.unsqueeze(0) + occ.unsqueeze(1) - co_occ)\n",
    "    jac = jac.nan_to_num(0)\n",
    "    jaccard.append(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Cosine Sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 4 # 30\n",
    "sae = list(sae_group)[id]\n",
    "sae_lfreq = log_frequencies[id]\n",
    "hyp = sae.cfg\n",
    "print(f\"Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosims(sae, log_freq=None, threshold=1e-6):\n",
    "    feats = sae.W_dec.data\n",
    "    if log_freq is not None:\n",
    "        alive = log_freq > np.log10(threshold)\n",
    "        feats = feats[alive]\n",
    "    \n",
    "    normed_feats = nn.functional.normalize(feats, dim=1)\n",
    "    cosim_matrix = (\n",
    "        (\n",
    "            normed_feats\n",
    "            @ normed_feats.mT\n",
    "        )\n",
    "        .detach()\n",
    "        .cpu()\n",
    "    )\n",
    "    return cosim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering\n",
    "def seriation(Z,N,cur_index):\n",
    "    '''\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "            \n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index-N,0])\n",
    "        right = int(Z[cur_index-N,1])\n",
    "        return (seriation(Z,N,left) + seriation(Z,N,right))\n",
    "    \n",
    "def compute_serial_matrix(dist_mat,method=\"ward\"):\n",
    "    '''\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            - method = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarhical tree\n",
    "            - res_linkage is the hierarhical tree (dendrogram)\n",
    "        \n",
    "        compute_serial_matrix transforms a distance matrix into \n",
    "        a sorted distance matrix according to the order implied \n",
    "        by the hierarchical tree (dendrogram)\n",
    "    '''\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method,preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N-2)\n",
    "    seriated_dist = np.zeros((N,N))\n",
    "    a,b = np.triu_indices(N,k=1)\n",
    "    seriated_dist[a,b] = dist_mat[ [res_order[i] for i in a], [res_order[j] for j in b]]\n",
    "    seriated_dist[b,a] = seriated_dist[a,b]\n",
    "    \n",
    "    return seriated_dist, res_order, res_linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = get_cosims(sae,sae_lfreq)\n",
    "dist_matrix = torch.acos(cosim_matrix).nan_to_num(0) # distance on the unit sphere\n",
    "dist_matrix.fill_diagonal_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"average\"] #[\"ward\",\"single\",\"average\",\"complete\"]\n",
    "for method in methods:\n",
    "    print(\"Method:\\t\",method)\n",
    "    \n",
    "    ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(dist_matrix,method)\n",
    "    \n",
    "    # plt.pcolormesh(ordered_dist_mat)\n",
    "    # # plt.xlim([0,N])\n",
    "    # # plt.ylim([0,N])\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosim_matrix = get_cosims(sae,sae_lfreq)\n",
    "p_name = \"0.6\"\n",
    "# sorted_cosim_matrix = cosim_matrix.sort(descending=True, dim=-1)[0].cpu()\n",
    "sorted_cosim_matrix = np.cos(ordered_dist_mat)\n",
    "plt.imshow(sorted_cosim_matrix[:100,:100], cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"SAE features\")\n",
    "plt.title(f\"SAE Features Cosine Similarity (L{p_name})\")\n",
    "# plt.savefig(f\"images_{cfg.seed}/ffl{p_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mncs(sae, log_freq=None, threshold=None, mode=\"mean\"):\n",
    "    # mode is either \"mean\" for mean next cosine similarity\n",
    "    # or \"max\" for max next cosine similarity\n",
    "    cosim_matrix = get_cosims(sae, log_freq, threshold)\n",
    "    for i in range(cosim_matrix.shape[0]):\n",
    "        cosim_matrix[i,i] = 0\n",
    "    if mode==\"mean\":\n",
    "        return cosim_matrix.max(dim=0)[0].mean()\n",
    "    if mode==\"max\":\n",
    "        return cosim_matrix.max(dim=0)[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mncs(sae,sae_lfreq))\n",
    "print(mncs(sae,sae_lfreq, mode=\"max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_str = \"1w6fnqmj\"\n",
    "path = f\"/root/mats_sae_training/checkpoints/{uuid_str}/{file_strings[-1]}\"\n",
    "sae_group_l1 = torch.load(path)\n",
    "log_frequencies_l1 = torch.load(f\"/root/mats_sae_training/checkpoints/{uuid_str}/{file_strings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 0 #9\n",
    "sae_l1 = list(sae_group_l1)[id]\n",
    "sae_lfreq_l1 = log_frequencies_l1[id]\n",
    "hyp = sae_l1.cfg\n",
    "print(f\"Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = get_cosims(sae_l1,sae_lfreq_l1)\n",
    "dist_matrix_l1 = torch.acos(cosim_matrix).nan_to_num(0) # distance on the unit sphere\n",
    "dist_matrix_l1.fill_diagonal_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"average\"] #[\"ward\",\"single\",\"average\",\"complete\"]\n",
    "for method in methods:\n",
    "    print(\"Method:\\t\",method)\n",
    "    \n",
    "    ordered_dist_mat_l1, res_order_l1, res_linkage_l1 = compute_serial_matrix(dist_matrix_l1,method)\n",
    "    \n",
    "    # plt.pcolormesh(ordered_dist_mat)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = get_cosims(sae_l1,sae_lfreq_l1)\n",
    "p_name = \"1\"\n",
    "sorted_cosim_matrix = torch.cos(torch.tensor(ordered_dist_mat_l1))\n",
    "plt.imshow(sorted_cosim_matrix[:100,:100], cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"SAE features\")\n",
    "plt.title(f\"SAE Features Cosine Similarity (L{p_name})\")\n",
    "# plt.savefig(f\"images_{cfg.seed}/ffl{p_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster by size of blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = get_cosims(sae_l1,sae_lfreq_l1)\n",
    "p_name = \"1\"\n",
    "sorted_cosim_matrix = cosim_matrix.sort(descending=True, dim=-1)[0].cpu()\n",
    "plt.imshow(sorted_cosim_matrix[:100,:100], cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"SAE features\")\n",
    "plt.title(f\"SAE Features Cosine Similarity (L{p_name})\")\n",
    "# plt.savefig(f\"images_{cfg.seed}/ffl{p_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mncs(sae_l1,sae_lfreq_l1))\n",
    "print(mncs(sae_l1,sae_lfreq_l1, mode=\"max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted((sorted_cosim_matrix[:,:] > 0.98).sum(dim=-1).numpy())[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted((sorted_cosim_matrix[:,:] > 0.8).sum(dim=-1).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosim_matrix = cosim_matrix.sort(descending=True, dim=-1)[0].cpu()\n",
    "duplicated_feature = (cosim_matrix > 0.999).sum(dim=-1).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_cosim_matrix[duplicated_feature, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_picked_features = cosim_matrix[duplicated_feature].argsort(descending=True)[:30]\n",
    "cherry_picked_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosim_matrix = get_cosims(sae_l1,sae_lfreq_l1)\n",
    "p_name = \"1\"\n",
    "# sorted_cosim_matrix = cosim_matrix.sort(descending=True, dim=-1)[0].cpu()\n",
    "plt.imshow(cosim_matrix[cherry_picked_features][:,cherry_picked_features], cmap=\"PiYG\", vmin=-1, vmax=1)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Cosine Similarity', rotation=270)\n",
    "plt.xlabel(\"SAE features\")\n",
    "plt.ylabel(\"SAE features\")\n",
    "plt.title(f\"Cherrypicked SAE Features Cosine Similarity (L{p_name})\")\n",
    "# plt.savefig(f\"images_{cfg.seed}/ffl{p_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
